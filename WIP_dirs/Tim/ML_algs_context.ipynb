{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"futurama/8ch_futurama_binary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[[\"Fry\", \"Bender\", \"Leela\", \"Farnsworth\", \"Hermes\", \"Zoidberg\", \"Amy\", \"Zapp\", \"Other\"]]\n",
    "df_quotes = df[\"Quotes\"].values.astype('U')\n",
    "# X = df2.values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25550, 16851)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_features=None)\n",
    "\n",
    "# Transforms the data into a bag of words\n",
    "count_quotes = count_vec.fit_transform(df_quotes)\n",
    "count_quotes.shape\n",
    "\n",
    "# bag_of_words = count_vec.transform(df_quotes)\n",
    "\n",
    "# Print the first 10 features of the count_vec\n",
    "# print(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))\n",
    "# print(\"\\nEvery 3rd feature:\\n{}\".format(count_vec.get_feature_names()[::3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Vocabulary size: {}\".format(len(count_train.vocabulary_)))\n",
    "# print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer(smooth_idf=False, sublinear_tf=False, norm=None)\n",
    "quote_train_tf = tf_transformer.fit_transform(X=count_quotes)\n",
    "quote_train_tf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_matrix = csr_matrix(df2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_matrix_combined = hstack([quote_train_tf,df2_matrix]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_matrix_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_final = pd.DataFrame(df2_matrix_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_final[\"Characters\"] = df[\"Characters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_matrix_combined = pd.DataFrame(df2_matrix_combined)\n",
    "df2_matrix_combined['Quotes'] = df['Quotes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2_matrix_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2_final[\"Characters\"].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train = X_train['Quotes']\n",
    "test_test = X_test['Quotes']\n",
    "\n",
    "del X_train['Quotes']\n",
    "del X_test['Quotes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayesian:  0.2903882279273638\n"
     ]
    }
   ],
   "source": [
    "clf_nb = MultinomialNB().fit(X_train, y_train)\n",
    "predicted_nb = clf_nb.predict(X_test)\n",
    "print(\"Naive Bayesian: \", np.mean(predicted_nb == y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Huddled together in fear like lice in a burnin...\n",
       "1                      Whose that?  The Queen of England?\n",
       "2                                   That's not necessary.\n",
       "3                             You're back from the dead?!\n",
       "4       Probably not. But I love you and I'm gonna kic...\n",
       "5       A holophonor? Only a few people in the whole u...\n",
       "6                                            Yes, Doctor.\n",
       "7       A patriot of the highest order has volunteered...\n",
       "8       Ah, buck up, meatloaf. Bender'll take you out ...\n",
       "9           Uh, sorry, Bender, that's just a phone booth.\n",
       "10                                                    No!\n",
       "11      C'mon, Bender. If you're done scamming Beck, w...\n",
       "12      Warning, perform virus scan? I'm waiting for p...\n",
       "13                                    What are you doing?\n",
       "14      Of course I'm awake. You wouldn't stop waking ...\n",
       "15      Oh so just 'cause a robot wants to kill humans...\n",
       "16                     I told you, my name is Dr. Cahill.\n",
       "17      I'm in disguise 'cause I'm thinking of escapin...\n",
       "18      It's all thanks to you, Leela. You stood by me...\n",
       "19                                  Fry? Are you alright?\n",
       "20                Is Nibbler stoked for the big pet show?\n",
       "21                         Come to Daddy, sweetie-ookums.\n",
       "22                                               Not you.\n",
       "23      The shame is too great. It's time to end this!...\n",
       "24                                      Initiate snu-snu!\n",
       "25      Now this is why you gotta use cast-iron cookware.\n",
       "26      Yes, let's all talk about Leela's personal lif...\n",
       "27                                         Now we get it!\n",
       "28      Listen up, piggies! I want a hovercopter. And ...\n",
       "29      We're rolling in three, two... What? We're alr...\n",
       "                              ...                        \n",
       "6358    Then I have no choice but to do something so p...\n",
       "6359    Now put that greasy rat outside and we'll tow ...\n",
       "6360    I'm ready. I'll take that big purple dinosaur,...\n",
       "6361                                               Check!\n",
       "6362                            You die. Or as you put it\n",
       "6363                                           Brilliant!\n",
       "6364                              And you are outta here!\n",
       "6365                                    What?  Oh.  Okay.\n",
       "6366                                         Fry, face it\n",
       "6367                   And I, I could be an acting coach!\n",
       "6368    Alright, when we get to Earth. But please don'...\n",
       "6369                          Thanks for the help, Leela.\n",
       "6370                                                  Ow!\n",
       "6371                             Prepare to be surprised.\n",
       "6372    It seems to be working at only half-capacity, ...\n",
       "6373    Oh, if only we hadn't flown penguins to Pluto ...\n",
       "6374       Acting like a moron won't bring your dog back.\n",
       "6375          Does anyone else smell burning dragon beak?\n",
       "6376                                       Planet Express\n",
       "6377                           Let's all go to the lobby!\n",
       "6378                         Also, my shoe that fell off.\n",
       "6379    I was attacked by a vicious, motorised sled. T...\n",
       "6380    This Xmas Day session of court will come to or...\n",
       "6381                                                  Ew!\n",
       "6382                                     Sure thing, pal.\n",
       "6383    My God, I really can read minds. I have a supe...\n",
       "6384          Why are you cheering, Fry? You're not rich!\n",
       "6385    The Spiderians, though weak and woman-like on ...\n",
       "6386                                 I'll save you, Pops!\n",
       "6387    It's true. I disguised myself as a Femputer so...\n",
       "Name: Quotes, Length: 6388, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del test_test['index']\n",
    "# test_test = test_test.reset_index()\n",
    "test_test['Quotes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if len(test_test['Quotes'][i]) > 50:\n",
    "        print(predicted_nb[i], y_test[i], test_test['Quotes'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_sgq = SGDClassifier().fit(X_train, y_train)\n",
    "# predicted_sgd = clf_sgq.predict(X_test)\n",
    "# print(\"SGD: \", np.mean(predicted_sgd == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "# predicted_rf = clf_rf.predict(X_test)\n",
    "# print(\"Random Forest: \", np.mean(predicted_rf == y_test))\n",
    "\n",
    "# clf_lr = LogisticRegression().fit(X_train, y_train)\n",
    "# predicted_lr = clf_lr.predict(X_test)\n",
    "# print(\"Logistic Regression\", np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_lr = LogisticRegression().fit(X_train, y_train)\n",
    "# predicted_lr = clf_lr.predict(X_test)\n",
    "# print(\"Logistic Regression\", np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = KNeighborsClassifier().fit(X_train, y_train)\n",
    "# predicted = clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "# predicted = clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the above consolidated:\n",
    "\n",
    "\n",
    "def contextual_ml(weight, min_len, num_chars):\n",
    "    print(f'Weight: {weight}')\n",
    "    \n",
    "    df = pd.read_csv(f\"futurama/{num_chars}ch_futurama_binary_{weight}.csv\")\n",
    "    \n",
    "    mask = (df['Quotes'].str.len() >min_len)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    df2 = df[[\"Fry\", \"Bender\", \"Leela\", \"Farnsworth\", \"Hermes\", \"Zoidberg\", \"Amy\", \"Zapp\", \"Other\"]]\n",
    "    df_quotes = df[\"Quotes\"].values.astype('U')\n",
    "\n",
    "    count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                                ngram_range=(1, 1), max_features=None)\n",
    "\n",
    "    # Transforms the data into a bag of words\n",
    "    count_quotes = count_vec.fit_transform(df_quotes)\n",
    "    count_quotes.shape\n",
    "    \n",
    "#     tfidf params: smooth_idf=False, sublinear_tf=False, norm=None\n",
    "\n",
    "    tf_transformer = TfidfTransformer(smooth_idf=False, sublinear_tf=False, norm=None)\n",
    "    quote_train_tf = tf_transformer.fit_transform(X=count_quotes)\n",
    "    quote_train_tf.toarray()\n",
    "\n",
    "    df2_matrix = csr_matrix(df2.values)\n",
    "\n",
    "    df2_matrix_combined = hstack([quote_train_tf,df2_matrix]).toarray()\n",
    "\n",
    "    df2_final = pd.DataFrame(df2_matrix_combined)\n",
    "\n",
    "    df2_final[\"Characters\"] = df[\"Characters\"]\n",
    "\n",
    "    X = df2_matrix_combined\n",
    "    y = df2_final[\"Characters\"].values.astype('U')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "    clf_nb = MultinomialNB().fit(X_train, y_train)\n",
    "    predicted_nb = clf_nb.predict(X_test)\n",
    "    score_nb = np.mean(predicted_nb == y_test)\n",
    "    score_list_nb.append(score_nb)\n",
    "    print(\"Naive Bayesian:______\", np.mean(predicted_nb == y_test))\n",
    "\n",
    "    clf_sgq = SGDClassifier().fit(X_train, y_train)\n",
    "    predicted_sgd = clf_sgq.predict(X_test)\n",
    "    score_sgd = np.mean(predicted_sgd == y_test)\n",
    "    score_list_sgd.append(score_sgd)\n",
    "    print(\"SGD:_________________\", np.mean(predicted_sgd == y_test))\n",
    "\n",
    "    clf_rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "    predicted_rf = clf_rf.predict(X_test)\n",
    "    score_rf = np.mean(predicted_rf == y_test)\n",
    "    score_list_rf.append(score_rf)\n",
    "    print(\"Random Forest:_______\", np.mean(predicted_rf == y_test))\n",
    "\n",
    "    clf_lr = LogisticRegression().fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    score_lr = np.mean(predicted_lr == y_test)\n",
    "    score_list_lr.append(score_lr)\n",
    "    print(\"Logistic Regression:_\", score_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min length:  45\n",
      "Weight: 41\n",
      "Naive Bayesian:  0.25826513911620297\n",
      "SGD:  0.288379705400982\n",
      "Random Forest:  0.47823240589198035\n",
      "Logistic Regression 0.3934533551554828\n",
      "Weight: 51\n",
      "Naive Bayesian:  0.25564648117839606\n",
      "SGD:  0.3436988543371522\n",
      "Random Forest:  0.46808510638297873\n",
      "Logistic Regression 0.39181669394435353\n",
      "Weight: 61\n",
      "Naive Bayesian:  0.25859247135842883\n",
      "SGD:  0.2392798690671031\n",
      "Random Forest:  0.46939443535188213\n",
      "Logistic Regression 0.3888707037643208\n",
      "Weight: 71\n",
      "Naive Bayesian:  0.25499181669394433\n",
      "SGD:  0.1364975450081833\n",
      "Random Forest:  0.46775777414075287\n",
      "Logistic Regression 0.39247135842880526\n",
      "Weight: 81\n",
      "Naive Bayesian:  0.2563011456628478\n",
      "SGD:  0.20949263502454993\n",
      "Random Forest:  0.47103109656301145\n",
      "Logistic Regression 0.3941080196399345\n",
      "min length:  48\n",
      "Weight: 41\n",
      "Naive Bayesian:  0.2669432918395574\n",
      "SGD:  0.4215076071922545\n",
      "Random Forest:  0.4934301521438451\n",
      "Logistic Regression 0.4042185338865837\n",
      "Weight: 51\n",
      "Naive Bayesian:  0.27040110650069155\n",
      "SGD:  0.5200553250345782\n",
      "Random Forest:  0.49930843706777317\n",
      "Logistic Regression 0.40525587828492393\n",
      "Weight: 61\n",
      "Naive Bayesian:  0.27005532503457813\n",
      "SGD:  0.43291839557399725\n",
      "Random Forest:  0.5\n",
      "Logistic Regression 0.4062932226832642\n",
      "Weight: 71\n",
      "Naive Bayesian:  0.2717842323651452\n",
      "SGD:  0.39349930843706776\n",
      "Random Forest:  0.4975795297372061\n",
      "Logistic Regression 0.40560165975103735\n",
      "Weight: 81\n",
      "Naive Bayesian:  0.27420470262793917\n",
      "SGD:  0.4789073305670816\n",
      "Random Forest:  0.4872060857538036\n",
      "Logistic Regression 0.40560165975103735\n",
      "min length:  51\n",
      "Weight: 41\n",
      "Naive Bayesian:  0.27706422018348625\n",
      "SGD:  0.4814678899082569\n",
      "Random Forest:  0.5394495412844037\n",
      "Logistic Regression 0.45541284403669724\n",
      "Weight: 51\n",
      "Naive Bayesian:  0.28110091743119264\n",
      "SGD:  0.4278899082568807\n",
      "Random Forest:  0.5291743119266055\n",
      "Logistic Regression 0.4557798165137615\n",
      "Weight: 61\n",
      "Naive Bayesian:  0.2822018348623853\n",
      "SGD:  0.4234862385321101\n",
      "Random Forest:  0.544954128440367\n",
      "Logistic Regression 0.45541284403669724\n",
      "Weight: 71\n",
      "Naive Bayesian:  0.28440366972477066\n",
      "SGD:  0.49394495412844036\n",
      "Random Forest:  0.5387155963302752\n",
      "Logistic Regression 0.45541284403669724\n",
      "Weight: 81\n",
      "Naive Bayesian:  0.2877064220183486\n",
      "SGD:  0.36660550458715596\n",
      "Random Forest:  0.551559633027523\n",
      "Logistic Regression 0.4557798165137615\n",
      "min length:  54\n",
      "Weight: 41\n",
      "Naive Bayesian:  0.2833268558103381\n",
      "SGD:  0.5549941702293043\n",
      "Random Forest:  0.5542168674698795\n",
      "Logistic Regression 0.4741546832491255\n",
      "Weight: 51\n",
      "Naive Bayesian:  0.2860474154683249\n",
      "SGD:  0.26039642440730665\n",
      "Random Forest:  0.567431014380101\n",
      "Logistic Regression 0.47376603186941313\n",
      "Weight: 61\n",
      "Naive Bayesian:  0.2883793237465993\n",
      "SGD:  0.388651379712398\n",
      "Random Forest:  0.5499417022930432\n",
      "Logistic Regression 0.491255343956471\n",
      "Weight: 71\n",
      "Naive Bayesian:  0.28721336960746213\n",
      "SGD:  0.4003109211037699\n",
      "Random Forest:  0.5557714729887291\n",
      "Logistic Regression 0.4842596191216479\n",
      "Weight: 81\n",
      "Naive Bayesian:  0.2868247182277497\n",
      "SGD:  0.35639331519626893\n",
      "Random Forest:  0.5584920326467159\n",
      "Logistic Regression 0.4815390594636611\n"
     ]
    }
   ],
   "source": [
    "score_list_nb = []\n",
    "score_list_sgd = []\n",
    "score_list_rf = []\n",
    "score_list_lr = []\n",
    "\n",
    "for i in range(45, 56, 3):\n",
    "    print(\"min length: \", i)\n",
    "    \n",
    "    for j in range(40, 85, 10):\n",
    "        j += 1\n",
    "        contextual_ml(j, i, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  17\n",
      "Max:    0.2883793237465993\n",
      "Avg:    0.27407272297291946\n"
     ]
    }
   ],
   "source": [
    "print(\"Index: \", score_list_nb.index(max(score_list_nb)))\n",
    "print(\"Max:   \", max(score_list_nb))\n",
    "print(\"Avg:   \", np.mean(score_list_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  15\n",
      "Max:    0.5549941702293043\n",
      "Avg:    0.38091886408524206\n"
     ]
    }
   ],
   "source": [
    "print(\"Index: \", score_list_sgd.index(max(score_list_sgd)))\n",
    "print(\"Max:   \", max(score_list_sgd))\n",
    "print(\"Avg:   \", np.mean(score_list_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  16\n",
      "Max:    0.567431014380101\n",
      "Avg:    0.5160865661910439\n"
     ]
    }
   ],
   "source": [
    "print(\"Index: \", score_list_rf.index(max(score_list_rf)))\n",
    "print(\"Max:   \", max(score_list_rf))\n",
    "print(\"Avg:   \", np.mean(score_list_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  17\n",
      "Max:    0.491255343956471\n",
      "Avg:    0.4335231994043838\n"
     ]
    }
   ],
   "source": [
    "print(\"Index: \", score_list_lr.index(max(score_list_lr)))\n",
    "print(\"Max:   \", max(score_list_lr))\n",
    "print(\"Avg:   \", np.mean(score_list_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the above consolidated:\n",
    "\n",
    "\n",
    "def contextual_ml2(weight, min_len, num_chars):\n",
    "    print(f'Weight: {weight}')\n",
    "    \n",
    "    df = pd.read_csv(f\"futurama/{num_chars}ch_futurama_binary_{weight}.csv\")\n",
    "    \n",
    "    mask = (df['Quotes'].str.len() >min_len)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    df2 = df[[\"Fry\", \"Bender\", \"Leela\", \"Farnsworth\", \"Hermes\", \"Zoidberg\", \"Amy\", \"Zapp\", \"Other\"]]\n",
    "    df_quotes = df[\"Quotes\"].values.astype('U')\n",
    "\n",
    "    count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                                ngram_range=(1, 1), max_features=None)\n",
    "\n",
    "    # Transforms the data into a bag of words\n",
    "    count_quotes = count_vec.fit_transform(df_quotes)\n",
    "    count_quotes.shape\n",
    "    \n",
    "#     tfidf params: smooth_idf=False, sublinear_tf=False, norm=None\n",
    "\n",
    "#     tf_transformer = TfidfTransformer(smooth_idf=False, sublinear_tf=False, norm=None)\n",
    "#     quote_train_tf = tf_transformer.fit_transform(X=count_quotes)\n",
    "#     quote_train_tf.toarray()\n",
    "\n",
    "    df2_matrix = csr_matrix(df2.values)\n",
    "\n",
    "    df2_matrix_combined = hstack([count_quotes,df2_matrix]).toarray()\n",
    "\n",
    "    df2_final = pd.DataFrame(df2_matrix_combined)\n",
    "\n",
    "    df2_final[\"Characters\"] = df[\"Characters\"]\n",
    "\n",
    "    X = df2_matrix_combined\n",
    "    y = df2_final[\"Characters\"].values.astype('U')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "    clf_nb = MultinomialNB().fit(X_train, y_train)\n",
    "    predicted_nb = clf_nb.predict(X_test)\n",
    "    score_nb = np.mean(predicted_nb == y_test)\n",
    "    score_list_nb.append(score_nb)\n",
    "    print(\"Naive Bayesian:______\", np.mean(predicted_nb == y_test))\n",
    "\n",
    "    clf_sgq = SGDClassifier().fit(X_train, y_train)\n",
    "    predicted_sgd = clf_sgq.predict(X_test)\n",
    "    score_sgd = np.mean(predicted_sgd == y_test)\n",
    "    score_list_sgd.append(score_sgd)\n",
    "    print(\"SGD:_________________\", np.mean(predicted_sgd == y_test))\n",
    "\n",
    "    clf_rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "    predicted_rf = clf_rf.predict(X_test)\n",
    "    score_rf = np.mean(predicted_rf == y_test)\n",
    "    score_list_rf.append(score_rf)\n",
    "    print(\"Random Forest:_______\", np.mean(predicted_rf == y_test))\n",
    "\n",
    "    clf_lr = LogisticRegression().fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    score_lr = np.mean(predicted_lr == y_test)\n",
    "    score_list_lr.append(score_lr)\n",
    "    print(\"Logistic Regression:_\", score_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min length:  10\n",
      "Weight: 1\n",
      "Naive Bayesian:  0.16166201117318435\n",
      "SGD:  0.1895949720670391\n",
      "Random Forest:  0.23550977653631286\n",
      "Logistic Regression 0.22119413407821228\n",
      "Weight: 11\n",
      "Naive Bayesian:  0.16934357541899442\n",
      "SGD:  0.18261173184357543\n",
      "Random Forest:  0.23533519553072627\n",
      "Logistic Regression 0.22136871508379888\n",
      "Weight: 21\n",
      "Naive Bayesian:  0.17108938547486033\n",
      "SGD:  0.16131284916201116\n",
      "Random Forest:  0.2431913407821229\n",
      "Logistic Regression 0.22206703910614525\n",
      "Weight: 31\n",
      "Naive Bayesian:  0.16847067039106145\n",
      "SGD:  0.21386173184357543\n",
      "Random Forest:  0.23830307262569833\n",
      "Logistic Regression 0.2210195530726257\n",
      "Weight: 41\n",
      "Naive Bayesian:  0.16777234636871508\n",
      "SGD:  0.26222067039106145\n",
      "Random Forest:  0.236731843575419\n",
      "Logistic Regression 0.2208449720670391\n",
      "Weight: 51\n",
      "Naive Bayesian:  0.16567737430167598\n",
      "SGD:  0.09427374301675978\n",
      "Random Forest:  0.23289106145251395\n",
      "Logistic Regression 0.22416201117318435\n",
      "min length:  20\n",
      "Weight: 1\n",
      "Naive Bayesian:  0.15129301567908776\n",
      "SGD:  0.1962940338016697\n",
      "Random Forest:  0.23131745062105477\n",
      "Logistic Regression 0.19853390348197922\n",
      "Weight: 11\n",
      "Naive Bayesian:  0.16208511504785175\n",
      "SGD:  0.13581755243331298\n",
      "Random Forest:  0.2250050906129098\n",
      "Logistic Regression 0.2056607615556913\n",
      "Weight: 21\n",
      "Naive Bayesian:  0.16065974343310935\n",
      "SGD:  0.24984728161270617\n",
      "Random Forest:  0.23070657707187944\n",
      "Logistic Regression 0.20667888413765018\n",
      "Weight: 31\n",
      "Naive Bayesian:  0.16208511504785175\n",
      "SGD:  0.15577275503970678\n",
      "Random Forest:  0.22520871512930157\n",
      "Logistic Regression 0.20606801058847485\n",
      "Weight: 41\n",
      "Naive Bayesian:  0.16045611891671757\n",
      "SGD:  0.1394827937283649\n",
      "Random Forest:  0.2333536957849725\n",
      "Logistic Regression 0.21054774994909387\n",
      "Weight: 51\n",
      "Naive Bayesian:  0.1610669924658929\n",
      "SGD:  0.1940541641213602\n",
      "Random Forest:  0.2180818570555895\n",
      "Logistic Regression 0.20749338220321728\n",
      "min length:  30\n",
      "Weight: 1\n",
      "Naive Bayesian:  0.17144256674014205\n",
      "SGD:  0.24687729610580456\n",
      "Random Forest:  0.3014939995101641\n",
      "Logistic Regression 0.25544942444281166\n",
      "Weight: 11\n",
      "Naive Bayesian:  0.18956649522409993\n",
      "SGD:  0.19397501836884642\n",
      "Random Forest:  0.2990448199853049\n",
      "Logistic Regression 0.2601028655400441\n",
      "Weight: 21\n",
      "Naive Bayesian:  0.1920156747489591\n",
      "SGD:  0.11168258633357825\n",
      "Random Forest:  0.2904726916482978\n",
      "Logistic Regression 0.26083761939750183\n",
      "Weight: 31\n",
      "Naive Bayesian:  0.1905461670340436\n",
      "SGD:  0.30933137398971344\n",
      "Random Forest:  0.29537105069801617\n",
      "Logistic Regression 0.26108253734998776\n",
      "Weight: 41\n",
      "Naive Bayesian:  0.19128092089150134\n",
      "SGD:  0.2890031839333823\n",
      "Random Forest:  0.29684055841293167\n",
      "Logistic Regression 0.26034778349253\n",
      "Weight: 51\n",
      "Naive Bayesian:  0.18981141317658584\n",
      "SGD:  0.3095762919421994\n",
      "Random Forest:  0.2916972814107274\n",
      "Logistic Regression 0.26083761939750183\n",
      "min length:  40\n",
      "Weight: 1\n",
      "Naive Bayesian:  0.20328358208955224\n",
      "SGD:  0.31791044776119404\n",
      "Random Forest:  0.4208955223880597\n",
      "Logistic Regression 0.3319402985074627\n",
      "Weight: 11\n",
      "Naive Bayesian:  0.21970149253731344\n",
      "SGD:  0.41492537313432837\n",
      "Random Forest:  0.4217910447761194\n",
      "Logistic Regression 0.32925373134328356\n",
      "Weight: 21\n",
      "Naive Bayesian:  0.2208955223880597\n",
      "SGD:  0.21164179104477612\n",
      "Random Forest:  0.4256716417910448\n",
      "Logistic Regression 0.3295522388059702\n",
      "Weight: 31\n",
      "Naive Bayesian:  0.2253731343283582\n",
      "SGD:  0.3805970149253731\n",
      "Random Forest:  0.42388059701492536\n",
      "Logistic Regression 0.32925373134328356\n",
      "Weight: 41\n",
      "Naive Bayesian:  0.2253731343283582\n",
      "SGD:  0.11432835820895522\n",
      "Random Forest:  0.41582089552238805\n",
      "Logistic Regression 0.328955223880597\n",
      "Weight: 51\n",
      "Naive Bayesian:  0.2244776119402985\n",
      "SGD:  0.4635820895522388\n",
      "Random Forest:  0.4262686567164179\n",
      "Logistic Regression 0.32985074626865674\n",
      "min length:  50\n",
      "Weight: 1\n",
      "Naive Bayesian:  0.23643550125763566\n",
      "SGD:  0.40280273086597196\n",
      "Random Forest:  0.5156306144448437\n",
      "Logistic Regression 0.42651814588573483\n",
      "Weight: 11\n",
      "Naive Bayesian:  0.2597915918074021\n",
      "SGD:  0.4800574919151994\n",
      "Random Forest:  0.5224577793747754\n",
      "Logistic Regression 0.4369385555156306\n",
      "Weight: 21\n",
      "Naive Bayesian:  0.2691340280273087\n",
      "SGD:  0.2971613366870284\n",
      "Random Forest:  0.5267696730147323\n",
      "Logistic Regression 0.44915558749550843\n",
      "Weight: 31\n",
      "Naive Bayesian:  0.2738052461372619\n",
      "SGD:  0.34746676248652536\n",
      "Random Forest:  0.5159899389148401\n",
      "Logistic Regression 0.45023356090549765\n",
      "Weight: 41\n",
      "Naive Bayesian:  0.2766798418972332\n",
      "SGD:  0.5379087315846209\n",
      "Random Forest:  0.518505210204815\n",
      "Logistic Regression 0.448796263025512\n",
      "Weight: 51\n",
      "Naive Bayesian:  0.27883578871721165\n",
      "SGD:  0.5102407473948976\n",
      "Random Forest:  0.5145526410348544\n",
      "Logistic Regression 0.43190801293568093\n"
     ]
    }
   ],
   "source": [
    "score_list_nb2 = []\n",
    "score_list_sgd2 = []\n",
    "score_list_rf2 = []\n",
    "score_list_lr2 = []\n",
    "\n",
    "for i in range(10, 51, 10):\n",
    "    print(\"min length: \", i)\n",
    "    \n",
    "    for j in range(0, 55, 10):\n",
    "        j += 1\n",
    "        contextual_ml(j, i, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  29\n",
      "Max:    0.27883578871721165\n",
      "Avg:    0.20000370589967756\n"
     ]
    }
   ],
   "source": [
    "print(\"Index: \", score_list_nb.index(max(score_list_nb)))\n",
    "print(\"Max:   \", max(score_list_nb))\n",
    "print(\"Avg:   \", np.mean(score_list_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  27\n",
      "Max:    0.5551563061444484\n",
      "Avg:    0.27677437419356843\n"
     ]
    }
   ],
   "source": [
    "print(\"Index: \", score_list_sgd.index(max(score_list_sgd)))\n",
    "print(\"Max:   \", max(score_list_sgd))\n",
    "print(\"Avg:   \", np.mean(score_list_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  56\n",
      "Max:    0.5267696730147323\n",
      "Avg:    0.33967600924603786\n"
     ]
    }
   ],
   "source": [
    "print(\"Index: \", score_list_rf.index(max(score_list_rf)))\n",
    "print(\"Max:   \", max(score_list_rf))\n",
    "print(\"Avg:   \", np.mean(score_list_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  27\n",
      "Max:    0.45023356090549765\n",
      "Avg:    0.2915551020676768\n"
     ]
    }
   ],
   "source": [
    "print(\"Index: \", score_list_lr.index(max(score_list_lr)))\n",
    "print(\"Max:   \", max(score_list_lr))\n",
    "print(\"Avg:   \", np.mean(score_list_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25550"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "0    25550\n",
      "\n",
      "Other         9754\n",
      "Fry           4014\n",
      "Bender        3734\n",
      "Leela         3194\n",
      "Farnsworth    1558\n",
      "Zoidberg       992\n",
      "Amy            933\n",
      "Hermes         867\n",
      "Zapp           504\n",
      "Name: Characters, dtype: int64\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "10    22910\n",
      "\n",
      "Other         8846\n",
      "Fry           3538\n",
      "Bender        3334\n",
      "Leela         2883\n",
      "Farnsworth    1427\n",
      "Zoidberg       858\n",
      "Hermes         783\n",
      "Amy            774\n",
      "Zapp           467\n",
      "Name: Characters, dtype: int64\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "20    19641\n",
      "\n",
      "Other         7622\n",
      "Fry           2962\n",
      "Bender        2809\n",
      "Leela         2488\n",
      "Farnsworth    1292\n",
      "Zoidberg       736\n",
      "Hermes         681\n",
      "Amy            637\n",
      "Zapp           414\n",
      "Name: Characters, dtype: int64\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "30    16332\n",
      "\n",
      "Other         6418\n",
      "Fry           2408\n",
      "Bender        2305\n",
      "Leela         2038\n",
      "Farnsworth    1154\n",
      "Zoidberg       588\n",
      "Hermes         562\n",
      "Amy            491\n",
      "Zapp           368\n",
      "Name: Characters, dtype: int64\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "40    13397\n",
      "\n",
      "Other         5324\n",
      "Fry           1909\n",
      "Bender        1854\n",
      "Leela         1669\n",
      "Farnsworth    1006\n",
      "Zoidberg       474\n",
      "Hermes         463\n",
      "Amy            371\n",
      "Zapp           327\n",
      "Name: Characters, dtype: int64\n",
      "--------------------\n",
      "\n",
      "--------------------\n",
      "50    11131\n",
      "\n",
      "Other         4479\n",
      "Fry           1533\n",
      "Bender        1490\n",
      "Leela         1369\n",
      "Farnsworth     903\n",
      "Hermes         385\n",
      "Zoidberg       378\n",
      "Zapp           300\n",
      "Amy            294\n",
      "Name: Characters, dtype: int64\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 51, 10):\n",
    "    mask = (df['Quotes'].str.len() >i)\n",
    "    masked_df = df.loc[mask]\n",
    "    \n",
    "    print('--------------------')\n",
    "    print(i, \"  \", len(masked_df))\n",
    "    print()\n",
    "    print(masked_df[\"Characters\"].value_counts())\n",
    "    print('--------------------')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
