{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"futurama/8ch_futurama_binary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df[[\"Fry\", \"Bender\", \"Leela\", \"Farnsworth\", \"Hermes\", \"Zoidberg\", \"Amy\", \"Zapp\", \"Other\"]]\n",
    "# df_quotes = df[\"Quotes\"].values.astype('U')\n",
    "# # X = df2.values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a CountVectorizer object: count_vectorizer\n",
    "# count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "#                             ngram_range=(1, 1), max_features=None)\n",
    "\n",
    "# # Transforms the data into a bag of words\n",
    "# count_quotes = count_vec.fit_transform(df_quotes)\n",
    "# count_quotes.shape\n",
    "\n",
    "# # bag_of_words = count_vec.transform(df_quotes)\n",
    "\n",
    "# # Print the first 10 features of the count_vec\n",
    "# # print(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))\n",
    "# # print(\"\\nEvery 3rd feature:\\n{}\".format(count_vec.get_feature_names()[::3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Vocabulary size: {}\".format(len(count_train.vocabulary_)))\n",
    "# print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_transformer = TfidfTransformer(smooth_idf=False, sublinear_tf=False, norm=None)\n",
    "# quote_train_tf = tf_transformer.fit_transform(X=count_quotes)\n",
    "# quote_train_tf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_matrix = csr_matrix(df2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_matrix_combined = hstack([quote_train_tf,df2_matrix]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_matrix_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_final = pd.DataFrame(df2_matrix_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_final[\"Characters\"] = df[\"Characters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df2_matrix_combined\n",
    "# y = df2_final[\"Characters\"].values.astype('U')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_nb = MultinomialNB().fit(X_train, y_train)\n",
    "# predicted_nb = clf_nb.predict(X_test)\n",
    "# print(\"Naive Bayesian: \", np.mean(predicted_nb == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_sgq = SGDClassifier().fit(X_train, y_train)\n",
    "# predicted_sgd = clf_sgq.predict(X_test)\n",
    "# print(\"SGD: \", np.mean(predicted_sgd == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "# predicted_rf = clf_rf.predict(X_test)\n",
    "# print(\"Random Forest: \", np.mean(predicted_rf == y_test))\n",
    "\n",
    "# clf_lr = LogisticRegression().fit(X_train, y_train)\n",
    "# predicted_lr = clf_lr.predict(X_test)\n",
    "# print(\"Logistic Regression\", np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_lr = LogisticRegression().fit(X_train, y_train)\n",
    "# predicted_lr = clf_lr.predict(X_test)\n",
    "# print(\"Logistic Regression\", np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = KNeighborsClassifier().fit(X_train, y_train)\n",
    "# predicted = clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "# predicted = clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the above consolidated:\n",
    "\n",
    "\n",
    "def contextual_ml(number):\n",
    "    print(f'Weight: {number}')\n",
    "    \n",
    "    df = pd.read_csv(f\"futurama/8ch_futurama_binary_{number}.csv\")\n",
    "\n",
    "    df2 = df[[\"Fry\", \"Bender\", \"Leela\", \"Farnsworth\", \"Hermes\", \"Zoidberg\", \"Amy\", \"Zapp\", \"Other\"]]\n",
    "    df_quotes = df[\"Quotes\"].values.astype('U')\n",
    "\n",
    "    count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                                ngram_range=(1, 2), max_features=None)\n",
    "\n",
    "    # Transforms the data into a bag of words\n",
    "    count_quotes = count_vec.fit_transform(df_quotes)\n",
    "    count_quotes.shape\n",
    "    \n",
    "#     tfidf params: smooth_idf=False, sublinear_tf=False, norm=None\n",
    "\n",
    "    tf_transformer = TfidfTransformer(smooth_idf=False, sublinear_tf=False, norm=None)\n",
    "    quote_train_tf = tf_transformer.fit_transform(X=count_quotes)\n",
    "    quote_train_tf.toarray()\n",
    "\n",
    "    df2_matrix = csr_matrix(df2.values)\n",
    "\n",
    "    df2_matrix_combined = hstack([quote_train_tf,df2_matrix]).toarray()\n",
    "\n",
    "    df2_final = pd.DataFrame(df2_matrix_combined)\n",
    "\n",
    "    df2_final[\"Characters\"] = df[\"Characters\"]\n",
    "\n",
    "    X = df2_matrix_combined\n",
    "    y = df2_final[\"Characters\"].values.astype('U')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "    clf_nb = MultinomialNB().fit(X_train, y_train)\n",
    "    predicted_nb = clf_nb.predict(X_test)\n",
    "    score_nb = np.mean(predicted_nb == y_test)\n",
    "    score_list_nb.append(score_nb)\n",
    "    print(\"Naive Bayesian: \", np.mean(predicted_nb == y_test))\n",
    "\n",
    "    clf_sgq = SGDClassifier().fit(X_train, y_train)\n",
    "    predicted_sgd = clf_sgq.predict(X_test)\n",
    "    score_sgd = np.mean(predicted_sgd == y_test)\n",
    "    score_list_sgd.append(score_sgd)\n",
    "    print(\"SGD: \", np.mean(predicted_sgd == y_test))\n",
    "\n",
    "    clf_rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "    predicted_rf = clf_rf.predict(X_test)\n",
    "    score_rf = np.mean(predicted_rf == y_test)\n",
    "    score_list_rf.append(score_rf)\n",
    "    print(\"Random Forest: \", np.mean(predicted_rf == y_test))\n",
    "\n",
    "    clf_lr = LogisticRegression().fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    score_lr = np.mean(predicted_lr == y_test)\n",
    "    score_list_lr.append(score_lr)\n",
    "    print(\"Logistic Regression\", score_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 51\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ad5faec54453>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mcontextual_ml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-9f3ff12fc8b5>\u001b[0m in \u001b[0;36mcontextual_ml\u001b[1;34m(number)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2_matrix_combined\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Characters\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'U'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mclf_nb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2059\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2059\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[1;34m(X, indices)\u001b[0m\n\u001b[0;32m    158\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score_list_nb = []\n",
    "score_list_sgd = []\n",
    "score_list_rf = []\n",
    "score_list_lr = []\n",
    "\n",
    "\n",
    "for i in range(50, 150, 10):\n",
    "    i += 1\n",
    "    contextual_ml(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
